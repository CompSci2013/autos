# AUTOS Development Environment Setup Procedure

**Document Version:** 1.0  
**Date:** 2025-10-13  
**Purpose:** Clean procedure to tear down and rebuild AUTOS development environment from scratch

---

## Overview

This document provides the verified steps to:
1. Remove all existing AUTOS containers and images
2. Rebuild and deploy the backend to Kubernetes
3. Verify backend deployment
4. Rebuild and start the frontend dev container
5. Verify site compilation

**Prerequisites:**
- K3s cluster running with Traefik ingress
- Elasticsearch service available at `elasticsearch.data.svc.cluster.local:9200`
- AUTOS namespace exists in Kubernetes
- Podman installed for container builds
- kubectl configured for cluster access

---

## Phase 1: Complete Cleanup

### Step 1: Stop and Remove All AUTOS Containers

**Server:** Thor

```bash
cd /home/odin/projects/autos
podman stop autos-backend-dev autos-frontend-dev
podman rm autos-backend-dev autos-frontend-dev
```

**Expected Output:** Container names confirming removal (SIGKILL warnings are normal)

---

### Step 2: Remove All AUTOS Images from Podman

**Server:** Thor

```bash
cd /home/odin/projects/autos
podman images | grep autos
```

**Note:** Record all image names and tags shown

```bash
# Remove all listed images (example command, adjust to your images)
podman rmi localhost/autos-backend:v1.2.1 localhost/autos-frontend:dev
```

**Expected Output:** "Untagged" and "Deleted" messages for each image

---

### Step 3: Remove All AUTOS Images from K3s

**Server:** Thor

```bash
cd /home/odin/projects/autos
sudo k3s ctr images list | grep autos
```

**Note:** Record all image names shown

```bash
# Remove all listed images (example command, adjust to your images)
sudo k3s ctr images rm localhost/autos-backend:v1.2.1 localhost/autos-frontend:dev
```

**Expected Output:** Image names confirming removal

---

### Step 4: Verify Complete Cleanup

**Server:** Thor

```bash
# Verify no autos images in K3s
sudo k3s ctr images list | grep autos
# Expected: No output

# Verify no autos images in Podman
podman images | grep autos
# Expected: No output
```

---

### Step 5: Scale Down Kubernetes Deployments

**Server:** Thor

```bash
cd /home/odin/projects/autos
kubectl scale deployment autos-backend autos-frontend --replicas=0 -n autos
kubectl get pods -n autos
```

**Expected Output:** "No resources found in autos namespace"

---

### Step 6: Remove Old Tar Archives

**Server:** Thor

```bash
# Backend directory
cd /home/odin/projects/autos/backend
ls -lh *.tar 2>/dev/null
rm *.tar 2>/dev/null

# Frontend directory
cd /home/odin/projects/autos/frontend
ls -lh *.tar 2>/dev/null
rm *.tar 2>/dev/null
```

**Expected Output:** "No tar files found" when re-listing

---

## Phase 2: Rebuild and Deploy Backend

### Step 7: Build Backend Image

**Server:** Thor

```bash
cd /home/odin/projects/autos/backend
podman build -t localhost/autos-backend:v1.2.1 .
```

**Expected Output:** 
- "STEP 1/7" through "STEP 7/7"
- "Successfully tagged localhost/autos-backend:v1.2.1"
- Final image hash

**Build Time:** ~1-2 minutes with clean cache

---

### Step 8: Export Backend Image

**Server:** Thor

```bash
cd /home/odin/projects/autos/backend
podman save localhost/autos-backend:v1.2.1 -o autos-backend-v1.2.1.tar
```

**Expected Output:** 
- "Copying blob" messages
- "Copying config" message
- "Writing manifest to image destination"

---

### Step 9: Import Backend Image to K3s

**Server:** Thor

```bash
cd /home/odin/projects/autos/backend
sudo k3s ctr images import autos-backend-v1.2.1.tar
```

**Expected Output:** 
- "localhost/autos-backend:v1.2.1 saved"
- Import completion with timing

---

### Step 10: Verify Backend Image in K3s

**Server:** Thor

```bash
sudo k3s ctr images list | grep autos-backend
```

**Expected Output:** One line showing `localhost/autos-backend:v1.2.1` with size ~157 MiB

---

### Step 11: Scale Up Backend Deployment

**Server:** Thor

```bash
cd /home/odin/projects/autos
kubectl scale deployment autos-backend --replicas=2 -n autos
```

**Expected Output:** "deployment.apps/autos-backend scaled"

---

### Step 12: Watch Backend Pods Start

**Server:** Thor

```bash
kubectl get pods -n autos -w
```

**Expected Output:** 
- Two pods transitioning to "1/1 Running" status
- Press `Ctrl+C` once both are running

**Startup Time:** ~10-30 seconds

---

### Step 13: Verify Backend Health

**Server:** Thor

```bash
# Test internal health endpoint
kubectl run -n autos curl-test --image=curlimages/curl:latest --rm -it --restart=Never -- curl http://autos-backend:3000/health
```

**Expected Output:** 
```json
{"status":"ok","service":"autos-backend","timestamp":"2025-10-13T..."}
```

---

### Step 14: Verify Backend API Through Ingress

**Server:** Thor

```bash
curl http://autos.minilab/api/v1/manufacturer-model-combinations?size=2 | jq
```

**Expected Output:** 
- JSON response with manufacturer-model data
- HTTP 200 status
- Data array with results

**This confirms:**
- ✓ Backend pods are running
- ✓ Backend connects to Elasticsearch
- ✓ Ingress routing is working
- ✓ API endpoints are functional

---

## Phase 3: Rebuild Frontend Dev Container

### Step 15: Build Frontend Dev Image

**Server:** Thor

```bash
cd /home/odin/projects/autos/frontend
podman build -f Dockerfile.dev -t localhost/autos-frontend:dev .
```

**Expected Output:** 
- Multi-step build process
- "Successfully tagged localhost/autos-frontend:dev"

**Build Time:** ~1-3 minutes depending on cache

---

### Step 16: Start Frontend Dev Container

**Server:** Thor

```bash
cd /home/odin/projects/autos/frontend
podman run -d --name autos-frontend-dev --network host -v /home/odin/projects/autos/frontend:/app:z -w /app localhost/autos-frontend:dev
```

**Expected Output:** Container ID hash (64 characters)

---

### Step 17: Verify Container is Running

**Server:** Thor

```bash
podman ps | grep autos-frontend-dev
```

**Expected Output:** Line showing container status "Up" with `autos-frontend-dev` name

---

### Step 18: Start Angular Dev Server

**Server:** Thor

```bash
podman exec -it autos-frontend-dev npm start -- --host 0.0.0.0 --port 4200
```

**Expected Output:** 
- npm package installation messages (if first run)
- Angular CLI compilation output
- "✔ Browser application bundle generation complete"
- "** Angular Live Development Server is listening on 0.0.0.0:4200 **"
- Either successful compilation or list of compilation errors

**Note:** Compilation errors indicate code issues to be fixed, not environment setup problems.

---

## Verification Checklist

After completing all steps, verify:

- [ ] No autos containers in Podman: `podman ps -a | grep autos`
- [ ] No autos images in Podman: `podman images | grep autos` (dev container will show)
- [ ] Backend image in K3s: `sudo k3s ctr images list | grep autos-backend`
- [ ] Two backend pods running: `kubectl get pods -n autos`
- [ ] Backend health check passes: `kubectl run curl-test...`
- [ ] Backend API responds: `curl http://autos.minilab/api/v1/...`
- [ ] Frontend dev container running: `podman ps | grep autos-frontend-dev`
- [ ] Angular dev server compiles: Shows compilation output (success or errors)

---

## Access Points

After successful setup:

- **Frontend Dev Server:** http://localhost:4200 (on Thor) or http://thor:4200 (from network)
- **Backend API (via Ingress):** http://autos.minilab/api/v1/...
- **Backend Health (internal):** http://autos-backend.autos.svc.cluster.local:3000/health

---

## Key Configuration Details

### Backend Environment Variables
Located in: `k8s/backend-deployment.yaml`
```yaml
ELASTICSEARCH_URL: http://elasticsearch.data.svc.cluster.local:9200
ELASTICSEARCH_INDEX: autos-unified
NODE_ENV: production
PORT: 3000
```

### Frontend Dev Container
- **Base Image:** node:18-alpine
- **Volume Mount:** `/home/odin/projects/autos/frontend:/app:z` (SELinux compatible)
- **Network:** host (access backend at localhost:3000)
- **Working Directory:** /app
- **Stay-Alive Command:** `tail -f /dev/null`

### Ingress Routing
- `/api` → autos-backend:3000
- `/` → autos-frontend:80

---

## Troubleshooting

### Backend Pods Not Starting
```bash
kubectl describe pod -n autos <pod-name>
kubectl logs -n autos deployment/autos-backend
```

### Frontend Container Exits Immediately
```bash
podman logs autos-frontend-dev
# Ensure using Dockerfile.dev, not Dockerfile.prod
```

### Permission Denied on Volume Mount
```bash
# Ensure :z flag is present in volume mount
-v /home/odin/projects/autos/frontend:/app:z
```

### Image Not Found in K3s
```bash
# Verify image was imported
sudo k3s ctr images list | grep autos
# Re-import if needed
sudo k3s ctr images import <tar-file>
```

---

## Development Workflow

After setup is complete:

1. **Edit Files:** Use VS Code Remote-SSH to Thor
2. **Watch Compilation:** Terminal shows automatic recompilation on save
3. **Test Changes:** Browser at http://localhost:4200 or http://thor:4200
4. **Stop Dev Server:** `Ctrl+C` in terminal running npm start
5. **Restart Dev Server:** `podman exec -it autos-frontend-dev npm start -- --host 0.0.0.0 --port 4200`
6. **End Session:** 
   ```bash
   podman stop autos-frontend-dev
   podman rm autos-frontend-dev
   ```

---

## Notes

- **Backend runs in K8s:** Always deployed as Kubernetes pods
- **Frontend dev uses Podman:** Long-running container with exec for HMR
- **Images are separate:** Podman and K3s use different image stores
- **Always use :z flag:** Required for SELinux systems on volume mounts
- **Clean between sessions:** Remove old tar files and unused images

---

**Document maintained by:** odin + Claude  
**Last verified:** 2025-10-13  
**Next review:** After significant infrastructure changes